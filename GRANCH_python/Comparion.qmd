---
title: "R-Python Comparison"
format: html
editor: visual
---

```{python}
# helper functions from within the library
import helper
import importlib
import numpy as np
import torch 
import pandas as pd
import pickle
from pickle import FALSE, TRUE
from scipy.stats import norm 
import itertools
from itertools import repeat 
import scipy.stats as sts 
from torch.distributions import Normal  
import time 


# in library 
import init_params_tensor
import init_model_tensor
import main_sim_tensor
import compute_prob_tensor
import helper
import num_stab_help



importlib.reload(helper)
importlib.reload(compute_prob_tensor)
importlib.reload(init_params_tensor)
importlib.reload(init_model_tensor)
importlib.reload(main_sim_tensor)
importlib.reload(num_stab_help)



```

# Basic

```{python}
# initialize parameters
params = init_params_tensor.granch_params(
      grid_mu = torch.linspace(start = -1, end = 1, steps = 5),
      grid_sigma = torch.linspace(start = 0.001, end = 1.8, steps = 3), 
      grid_y = torch.linspace(start = -1, end = 1, steps = 3), 
      grid_epsilon =  torch.linspace(start = 0.001, end = 1.8, steps = 3), 
      hypothetical_obs_grid_n = 5, 
      mu_prior = 0.001,
      V_prior = 0.001, 
      alpha_prior = 1, 
      beta_prior = 1,
      epsilon  = 0.000001, 
      mu_epsilon = torch.tensor([0.001]), 
      sd_epsilon = torch.tensor([4]), 
      world_EIGs = 0.0001,
      max_observation = 500)

# add the various different cached bits
params.add_meshed_grid()
params.add_lp_mu_sigma()
params.add_y_given_mu_sigma()
params.add_lp_epsilon()
params.add_priors()

tensor_stimuli = init_model_tensor.granch_stimuli(1, 'BBBBBB')
tensor_model = init_model_tensor.granch_model(500, tensor_stimuli)


res = main_sim_tensor.granch_main_simulation(params, tensor_model, tensor_stimuli)
```


# Testing stabiliy

```{python}
def test_step_stability(grid_step, hypo_obs_step): 
  # initialize parameters
  start_time = time.perf_counter()
  params = init_params_tensor.granch_params(
      grid_mu = torch.linspace(start = -1, end = 1, steps = grid_step),
      grid_sigma = torch.linspace(start = 0.001, end = 1.8, steps = grid_step), 
      grid_y = torch.linspace(start = -1, end = 1, steps = grid_step), 
      grid_epsilon =  torch.linspace(start = 0.001, end = 1.8, steps = grid_step), 
      hypothetical_obs_grid_n = hypo_obs_step, 
      mu_prior = 0.001,
      V_prior = 0.001, 
      alpha_prior = 1, 
      beta_prior = 1,
      epsilon  = 0.000001, 
      mu_epsilon = torch.tensor([0.001]), 
      sd_epsilon = torch.tensor([4]), 
      world_EIGs = 0.0001,
      max_observation = 500)
  # add the various different cached bits
  params.add_meshed_grid()
  params.add_lp_mu_sigma()
  params.add_y_given_mu_sigma()
  params.add_lp_epsilon()
  params.add_priors()
  tensor_stimuli = init_model_tensor.granch_stimuli(1, 'BBBDBB')
  tensor_model = init_model_tensor.granch_model(500, tensor_stimuli)
  res = main_sim_tensor.granch_main_simulation(params, tensor_model, tensor_stimuli)
  end_time = time.perf_counter()
  t = end_time - start_time
  b = res.behavior
  b["grid_step"] = grid_step 
  b["hypo_obs_step"] = hypo_obs_step
  b["time"] = t
  return b 


  
```



```{python}

grid_step_n = pd.DataFrame({"grid_step":[10]})
hypo_step_n = pd.DataFrame({"hypo_obs_step": [5, 10, 15]})
grid_step_n["key"] = 0
hypo_step_n["key"] = 0

step_df = grid_step_n.merge(hypo_step_n, on = "key", how = "outer")
#print(step_df)
res_df = pd.DataFrame()
for i in range(0, len(step_df)): 
  temp_res = test_step_stability(step_df["grid_step"][i], step_df["hypo_obs_step"][i])
  temp_res.index.name = 't'
  temp_res.reset_index(inplace = True)
  res_df = pd.concat([res_df, temp_res])
print(res_df)
  
```



# Testing jittering 



```{python}
main_res = num_stab_help.run_jitter_simulation(grid_mu_start = -1, grid_mu_end = 1, grid_mu_step = 30,
              grid_sigma_start = 0.001, grid_sigma_end = 1.8,grid_sigma_step = 30, 
              grid_y_start = -1, grid_y_end = 1,grid_y_step = 30, 
              grid_epsilon_start = 0.001, grid_epsilon_end = 1.8, grid_epsilon_step = 30,
              jitter_range = 0.00001, jitter_n = 10)
```



```{python}
with open('res.pkl', 'rb') as inp:
    res2 = pickle.load(inp)
    
res2
```


# Plotting

```{r}
library(reticulate)
library(here)
library(tidyverse)

```

## eig
```{r}
py$main_res %>% 
  mutate(eig = unlist(EIG)) %>% 
  filter(!is.na(eig)) %>% 
  mutate(stimulus_id = unlist(stimulus_id)) %>% 
  group_by(stimulus_id, j_i) %>% 
  summarise(sample_n = n()) %>% 
  ggplot(aes(x = stimulus_id, y = sample_n)) + 
  geom_point(alpha = .3, position = position_jitter(width = .3)) + 
  geom_line() + 
  theme_classic()
```


```{r}
py$res_df %>% 
  filter(!is.na(EIG)) %>% 
  mutate(eig = unlist(EIG)) %>% 
  ggplot(aes(x = as.numeric(t), y = eig, color = as.factor(grid_step), group = as.factor(grid_step))) + 
  geom_point(position = position_jitter(width = 3), alpha = .3) +  
  theme_classic() + 
  facet_wrap(~hypo_obs_step)
```

## sample

```{r}
py$res_df %>% 
  filter(!is.na(EIG)) %>% 
  mutate(eig = unlist(EIG), 
         stimulus_id = unlist(stimulus_id)) %>% 
  group_by(stimulus_id, grid_step, hypo_obs_step, time) %>% 
  summarise(sample_n = n()) %>% 
  ggplot(aes(x = stimulus_id, y = sample_n, color = as.factor(grid_step), group = as.factor(grid_step))) + 
  geom_point(alpha = .3) + 
  geom_line() + 
  theme_classic() + 
  facet_wrap(~hypo_obs_step)
```

## time

```{r}
py$res_df %>% 
  filter(!is.na(EIG)) %>% 
  distinct(grid_step, hypo_obs_step, time) %>% 
  ggplot(aes(x = as.factor(grid_step), y = time, color = as.factor(hypo_obs_step))) + 
  geom_point(alpha = .3)+ 
  theme_classic() 
```
