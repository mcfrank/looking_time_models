{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'granch_utils.lesioned_sim' from '/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/granch_utils/lesioned_sim.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import pyro\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import importlib\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from granch_utils import init_model_tensor, main_sim_tensor, lesioned_sim, compute_prob_tensor, init_params_tensor, num_stab_help\n",
    "#importlib.reload(granch_utils)\n",
    "importlib.reload(num_stab_help)\n",
    "importlib.reload(init_model_tensor)\n",
    "importlib.reload(main_sim_tensor)\n",
    "importlib.reload(lesioned_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20, 20])\n",
      "torch.Size([10, 20, 20, 20])\n",
      "torch.Size([20, 20, 20])\n",
      "torch.Size([10, 20, 20, 20])\n",
      "torch.Size([20, 20, 20])\n",
      "torch.Size([10, 20, 20, 20])\n",
      "torch.Size([20, 20, 20])\n",
      "torch.Size([10, 20, 20, 20])\n",
      "torch.Size([20, 20, 20])\n",
      "torch.Size([10, 20, 20, 20])\n",
      "torch.Size([20, 20, 20])\n",
      "torch.Size([10, 20, 20, 20])\n",
      "    stimulus_id  EIG Look_away\n",
      "0             0 -0.0      True\n",
      "1             1 -0.0      True\n",
      "2             2 -0.0      True\n",
      "3             3 -0.0      True\n",
      "4             4  0.0      True\n",
      "5             5  0.0      True\n",
      "6           NaN  NaN       NaN\n",
      "7           NaN  NaN       NaN\n",
      "8           NaN  NaN       NaN\n",
      "9           NaN  NaN       NaN\n",
      "10          NaN  NaN       NaN\n",
      "11          NaN  NaN       NaN\n",
      "12          NaN  NaN       NaN\n",
      "13          NaN  NaN       NaN\n",
      "14          NaN  NaN       NaN\n",
      "15          NaN  NaN       NaN\n",
      "16          NaN  NaN       NaN\n",
      "17          NaN  NaN       NaN\n",
      "18          NaN  NaN       NaN\n",
      "19          NaN  NaN       NaN\n",
      "20          NaN  NaN       NaN\n",
      "21          NaN  NaN       NaN\n",
      "22          NaN  NaN       NaN\n",
      "23          NaN  NaN       NaN\n",
      "24          NaN  NaN       NaN\n",
      "25          NaN  NaN       NaN\n",
      "26          NaN  NaN       NaN\n",
      "27          NaN  NaN       NaN\n",
      "28          NaN  NaN       NaN\n",
      "29          NaN  NaN       NaN\n",
      "30          NaN  NaN       NaN\n",
      "31          NaN  NaN       NaN\n",
      "32          NaN  NaN       NaN\n",
      "33          NaN  NaN       NaN\n",
      "34          NaN  NaN       NaN\n",
      "35          NaN  NaN       NaN\n",
      "36          NaN  NaN       NaN\n",
      "37          NaN  NaN       NaN\n",
      "38          NaN  NaN       NaN\n",
      "39          NaN  NaN       NaN\n",
      "40          NaN  NaN       NaN\n",
      "41          NaN  NaN       NaN\n",
      "42          NaN  NaN       NaN\n",
      "43          NaN  NaN       NaN\n",
      "44          NaN  NaN       NaN\n",
      "45          NaN  NaN       NaN\n",
      "46          NaN  NaN       NaN\n",
      "47          NaN  NaN       NaN\n",
      "48          NaN  NaN       NaN\n",
      "49          NaN  NaN       NaN\n",
      "50          NaN  NaN       NaN\n",
      "51          NaN  NaN       NaN\n",
      "52          NaN  NaN       NaN\n",
      "53          NaN  NaN       NaN\n",
      "54          NaN  NaN       NaN\n",
      "55          NaN  NaN       NaN\n",
      "56          NaN  NaN       NaN\n",
      "57          NaN  NaN       NaN\n",
      "58          NaN  NaN       NaN\n",
      "59          NaN  NaN       NaN\n",
      "60          NaN  NaN       NaN\n",
      "61          NaN  NaN       NaN\n",
      "62          NaN  NaN       NaN\n",
      "63          NaN  NaN       NaN\n",
      "64          NaN  NaN       NaN\n",
      "65          NaN  NaN       NaN\n",
      "66          NaN  NaN       NaN\n",
      "67          NaN  NaN       NaN\n",
      "68          NaN  NaN       NaN\n",
      "69          NaN  NaN       NaN\n",
      "70          NaN  NaN       NaN\n",
      "71          NaN  NaN       NaN\n",
      "72          NaN  NaN       NaN\n",
      "73          NaN  NaN       NaN\n",
      "74          NaN  NaN       NaN\n",
      "75          NaN  NaN       NaN\n",
      "76          NaN  NaN       NaN\n",
      "77          NaN  NaN       NaN\n",
      "78          NaN  NaN       NaN\n",
      "79          NaN  NaN       NaN\n",
      "80          NaN  NaN       NaN\n",
      "81          NaN  NaN       NaN\n",
      "82          NaN  NaN       NaN\n",
      "83          NaN  NaN       NaN\n",
      "84          NaN  NaN       NaN\n",
      "85          NaN  NaN       NaN\n",
      "86          NaN  NaN       NaN\n",
      "87          NaN  NaN       NaN\n",
      "88          NaN  NaN       NaN\n",
      "89          NaN  NaN       NaN\n",
      "90          NaN  NaN       NaN\n",
      "91          NaN  NaN       NaN\n",
      "92          NaN  NaN       NaN\n",
      "93          NaN  NaN       NaN\n",
      "94          NaN  NaN       NaN\n",
      "95          NaN  NaN       NaN\n",
      "96          NaN  NaN       NaN\n",
      "97          NaN  NaN       NaN\n",
      "98          NaN  NaN       NaN\n",
      "99          NaN  NaN       NaN\n",
      "100         NaN  NaN       NaN\n",
      "101         NaN  NaN       NaN\n",
      "102         NaN  NaN       NaN\n",
      "103         NaN  NaN       NaN\n",
      "104         NaN  NaN       NaN\n",
      "105         NaN  NaN       NaN\n",
      "106         NaN  NaN       NaN\n",
      "107         NaN  NaN       NaN\n",
      "108         NaN  NaN       NaN\n",
      "109         NaN  NaN       NaN\n",
      "110         NaN  NaN       NaN\n",
      "111         NaN  NaN       NaN\n",
      "112         NaN  NaN       NaN\n",
      "113         NaN  NaN       NaN\n",
      "114         NaN  NaN       NaN\n",
      "115         NaN  NaN       NaN\n",
      "116         NaN  NaN       NaN\n",
      "117         NaN  NaN       NaN\n",
      "118         NaN  NaN       NaN\n",
      "119         NaN  NaN       NaN\n",
      "120         NaN  NaN       NaN\n",
      "121         NaN  NaN       NaN\n",
      "122         NaN  NaN       NaN\n",
      "123         NaN  NaN       NaN\n",
      "124         NaN  NaN       NaN\n",
      "125         NaN  NaN       NaN\n",
      "126         NaN  NaN       NaN\n",
      "127         NaN  NaN       NaN\n",
      "128         NaN  NaN       NaN\n",
      "129         NaN  NaN       NaN\n",
      "130         NaN  NaN       NaN\n",
      "131         NaN  NaN       NaN\n",
      "132         NaN  NaN       NaN\n",
      "133         NaN  NaN       NaN\n",
      "134         NaN  NaN       NaN\n",
      "135         NaN  NaN       NaN\n",
      "136         NaN  NaN       NaN\n",
      "137         NaN  NaN       NaN\n",
      "138         NaN  NaN       NaN\n",
      "139         NaN  NaN       NaN\n",
      "140         NaN  NaN       NaN\n",
      "141         NaN  NaN       NaN\n",
      "142         NaN  NaN       NaN\n",
      "143         NaN  NaN       NaN\n",
      "144         NaN  NaN       NaN\n",
      "145         NaN  NaN       NaN\n",
      "146         NaN  NaN       NaN\n",
      "147         NaN  NaN       NaN\n",
      "148         NaN  NaN       NaN\n",
      "149         NaN  NaN       NaN\n",
      "150         NaN  NaN       NaN\n",
      "151         NaN  NaN       NaN\n",
      "152         NaN  NaN       NaN\n",
      "153         NaN  NaN       NaN\n",
      "154         NaN  NaN       NaN\n",
      "155         NaN  NaN       NaN\n",
      "156         NaN  NaN       NaN\n",
      "157         NaN  NaN       NaN\n",
      "158         NaN  NaN       NaN\n",
      "159         NaN  NaN       NaN\n",
      "160         NaN  NaN       NaN\n",
      "161         NaN  NaN       NaN\n",
      "162         NaN  NaN       NaN\n",
      "163         NaN  NaN       NaN\n",
      "164         NaN  NaN       NaN\n",
      "165         NaN  NaN       NaN\n",
      "166         NaN  NaN       NaN\n",
      "167         NaN  NaN       NaN\n",
      "168         NaN  NaN       NaN\n",
      "169         NaN  NaN       NaN\n",
      "170         NaN  NaN       NaN\n",
      "171         NaN  NaN       NaN\n",
      "172         NaN  NaN       NaN\n",
      "173         NaN  NaN       NaN\n",
      "174         NaN  NaN       NaN\n",
      "175         NaN  NaN       NaN\n",
      "176         NaN  NaN       NaN\n",
      "177         NaN  NaN       NaN\n",
      "178         NaN  NaN       NaN\n",
      "179         NaN  NaN       NaN\n",
      "180         NaN  NaN       NaN\n",
      "181         NaN  NaN       NaN\n",
      "182         NaN  NaN       NaN\n",
      "183         NaN  NaN       NaN\n",
      "184         NaN  NaN       NaN\n",
      "185         NaN  NaN       NaN\n",
      "186         NaN  NaN       NaN\n",
      "187         NaN  NaN       NaN\n",
      "188         NaN  NaN       NaN\n",
      "189         NaN  NaN       NaN\n",
      "190         NaN  NaN       NaN\n",
      "191         NaN  NaN       NaN\n",
      "192         NaN  NaN       NaN\n",
      "193         NaN  NaN       NaN\n",
      "194         NaN  NaN       NaN\n",
      "195         NaN  NaN       NaN\n",
      "196         NaN  NaN       NaN\n",
      "197         NaN  NaN       NaN\n",
      "198         NaN  NaN       NaN\n",
      "199         NaN  NaN       NaN\n",
      "200         NaN  NaN       NaN\n",
      "201         NaN  NaN       NaN\n",
      "202         NaN  NaN       NaN\n",
      "203         NaN  NaN       NaN\n",
      "204         NaN  NaN       NaN\n",
      "205         NaN  NaN       NaN\n",
      "206         NaN  NaN       NaN\n",
      "207         NaN  NaN       NaN\n",
      "208         NaN  NaN       NaN\n",
      "209         NaN  NaN       NaN\n",
      "210         NaN  NaN       NaN\n",
      "211         NaN  NaN       NaN\n",
      "212         NaN  NaN       NaN\n",
      "213         NaN  NaN       NaN\n",
      "214         NaN  NaN       NaN\n",
      "215         NaN  NaN       NaN\n",
      "216         NaN  NaN       NaN\n",
      "217         NaN  NaN       NaN\n",
      "218         NaN  NaN       NaN\n",
      "219         NaN  NaN       NaN\n",
      "220         NaN  NaN       NaN\n",
      "221         NaN  NaN       NaN\n",
      "222         NaN  NaN       NaN\n",
      "223         NaN  NaN       NaN\n",
      "224         NaN  NaN       NaN\n",
      "225         NaN  NaN       NaN\n",
      "226         NaN  NaN       NaN\n",
      "227         NaN  NaN       NaN\n",
      "228         NaN  NaN       NaN\n",
      "229         NaN  NaN       NaN\n",
      "230         NaN  NaN       NaN\n",
      "231         NaN  NaN       NaN\n",
      "232         NaN  NaN       NaN\n",
      "233         NaN  NaN       NaN\n",
      "234         NaN  NaN       NaN\n",
      "235         NaN  NaN       NaN\n",
      "236         NaN  NaN       NaN\n",
      "237         NaN  NaN       NaN\n",
      "238         NaN  NaN       NaN\n",
      "239         NaN  NaN       NaN\n",
      "240         NaN  NaN       NaN\n",
      "241         NaN  NaN       NaN\n",
      "242         NaN  NaN       NaN\n",
      "243         NaN  NaN       NaN\n",
      "244         NaN  NaN       NaN\n",
      "245         NaN  NaN       NaN\n",
      "246         NaN  NaN       NaN\n",
      "247         NaN  NaN       NaN\n",
      "248         NaN  NaN       NaN\n",
      "249         NaN  NaN       NaN\n",
      "250         NaN  NaN       NaN\n",
      "251         NaN  NaN       NaN\n",
      "252         NaN  NaN       NaN\n",
      "253         NaN  NaN       NaN\n",
      "254         NaN  NaN       NaN\n",
      "255         NaN  NaN       NaN\n",
      "256         NaN  NaN       NaN\n",
      "257         NaN  NaN       NaN\n",
      "258         NaN  NaN       NaN\n",
      "259         NaN  NaN       NaN\n",
      "260         NaN  NaN       NaN\n",
      "261         NaN  NaN       NaN\n",
      "262         NaN  NaN       NaN\n",
      "263         NaN  NaN       NaN\n",
      "264         NaN  NaN       NaN\n",
      "265         NaN  NaN       NaN\n",
      "266         NaN  NaN       NaN\n",
      "267         NaN  NaN       NaN\n",
      "268         NaN  NaN       NaN\n",
      "269         NaN  NaN       NaN\n",
      "270         NaN  NaN       NaN\n",
      "271         NaN  NaN       NaN\n",
      "272         NaN  NaN       NaN\n",
      "273         NaN  NaN       NaN\n",
      "274         NaN  NaN       NaN\n",
      "275         NaN  NaN       NaN\n",
      "276         NaN  NaN       NaN\n",
      "277         NaN  NaN       NaN\n",
      "278         NaN  NaN       NaN\n",
      "279         NaN  NaN       NaN\n",
      "280         NaN  NaN       NaN\n",
      "281         NaN  NaN       NaN\n",
      "282         NaN  NaN       NaN\n",
      "283         NaN  NaN       NaN\n",
      "284         NaN  NaN       NaN\n",
      "285         NaN  NaN       NaN\n",
      "286         NaN  NaN       NaN\n",
      "287         NaN  NaN       NaN\n",
      "288         NaN  NaN       NaN\n",
      "289         NaN  NaN       NaN\n",
      "290         NaN  NaN       NaN\n",
      "291         NaN  NaN       NaN\n",
      "292         NaN  NaN       NaN\n",
      "293         NaN  NaN       NaN\n",
      "294         NaN  NaN       NaN\n",
      "295         NaN  NaN       NaN\n",
      "296         NaN  NaN       NaN\n",
      "297         NaN  NaN       NaN\n",
      "298         NaN  NaN       NaN\n",
      "299         NaN  NaN       NaN\n",
      "300         NaN  NaN       NaN\n",
      "301         NaN  NaN       NaN\n",
      "302         NaN  NaN       NaN\n",
      "303         NaN  NaN       NaN\n",
      "304         NaN  NaN       NaN\n",
      "305         NaN  NaN       NaN\n",
      "306         NaN  NaN       NaN\n",
      "307         NaN  NaN       NaN\n",
      "308         NaN  NaN       NaN\n",
      "309         NaN  NaN       NaN\n",
      "310         NaN  NaN       NaN\n",
      "311         NaN  NaN       NaN\n",
      "312         NaN  NaN       NaN\n",
      "313         NaN  NaN       NaN\n",
      "314         NaN  NaN       NaN\n",
      "315         NaN  NaN       NaN\n",
      "316         NaN  NaN       NaN\n",
      "317         NaN  NaN       NaN\n",
      "318         NaN  NaN       NaN\n",
      "319         NaN  NaN       NaN\n",
      "320         NaN  NaN       NaN\n",
      "321         NaN  NaN       NaN\n",
      "322         NaN  NaN       NaN\n",
      "323         NaN  NaN       NaN\n",
      "324         NaN  NaN       NaN\n",
      "325         NaN  NaN       NaN\n",
      "326         NaN  NaN       NaN\n",
      "327         NaN  NaN       NaN\n",
      "328         NaN  NaN       NaN\n",
      "329         NaN  NaN       NaN\n",
      "330         NaN  NaN       NaN\n",
      "331         NaN  NaN       NaN\n",
      "332         NaN  NaN       NaN\n",
      "333         NaN  NaN       NaN\n",
      "334         NaN  NaN       NaN\n",
      "335         NaN  NaN       NaN\n",
      "336         NaN  NaN       NaN\n",
      "337         NaN  NaN       NaN\n",
      "338         NaN  NaN       NaN\n",
      "339         NaN  NaN       NaN\n",
      "340         NaN  NaN       NaN\n",
      "341         NaN  NaN       NaN\n",
      "342         NaN  NaN       NaN\n",
      "343         NaN  NaN       NaN\n",
      "344         NaN  NaN       NaN\n",
      "345         NaN  NaN       NaN\n",
      "346         NaN  NaN       NaN\n",
      "347         NaN  NaN       NaN\n",
      "348         NaN  NaN       NaN\n",
      "349         NaN  NaN       NaN\n",
      "350         NaN  NaN       NaN\n",
      "351         NaN  NaN       NaN\n",
      "352         NaN  NaN       NaN\n",
      "353         NaN  NaN       NaN\n",
      "354         NaN  NaN       NaN\n",
      "355         NaN  NaN       NaN\n",
      "356         NaN  NaN       NaN\n",
      "357         NaN  NaN       NaN\n",
      "358         NaN  NaN       NaN\n",
      "359         NaN  NaN       NaN\n",
      "360         NaN  NaN       NaN\n",
      "361         NaN  NaN       NaN\n",
      "362         NaN  NaN       NaN\n",
      "363         NaN  NaN       NaN\n",
      "364         NaN  NaN       NaN\n",
      "365         NaN  NaN       NaN\n",
      "366         NaN  NaN       NaN\n",
      "367         NaN  NaN       NaN\n",
      "368         NaN  NaN       NaN\n",
      "369         NaN  NaN       NaN\n",
      "370         NaN  NaN       NaN\n",
      "371         NaN  NaN       NaN\n",
      "372         NaN  NaN       NaN\n",
      "373         NaN  NaN       NaN\n",
      "374         NaN  NaN       NaN\n",
      "375         NaN  NaN       NaN\n",
      "376         NaN  NaN       NaN\n",
      "377         NaN  NaN       NaN\n",
      "378         NaN  NaN       NaN\n",
      "379         NaN  NaN       NaN\n",
      "380         NaN  NaN       NaN\n",
      "381         NaN  NaN       NaN\n",
      "382         NaN  NaN       NaN\n",
      "383         NaN  NaN       NaN\n",
      "384         NaN  NaN       NaN\n",
      "385         NaN  NaN       NaN\n",
      "386         NaN  NaN       NaN\n",
      "387         NaN  NaN       NaN\n",
      "388         NaN  NaN       NaN\n",
      "389         NaN  NaN       NaN\n",
      "390         NaN  NaN       NaN\n",
      "391         NaN  NaN       NaN\n",
      "392         NaN  NaN       NaN\n",
      "393         NaN  NaN       NaN\n",
      "394         NaN  NaN       NaN\n",
      "395         NaN  NaN       NaN\n",
      "396         NaN  NaN       NaN\n",
      "397         NaN  NaN       NaN\n",
      "398         NaN  NaN       NaN\n",
      "399         NaN  NaN       NaN\n",
      "400         NaN  NaN       NaN\n",
      "401         NaN  NaN       NaN\n",
      "402         NaN  NaN       NaN\n",
      "403         NaN  NaN       NaN\n",
      "404         NaN  NaN       NaN\n",
      "405         NaN  NaN       NaN\n",
      "406         NaN  NaN       NaN\n",
      "407         NaN  NaN       NaN\n",
      "408         NaN  NaN       NaN\n",
      "409         NaN  NaN       NaN\n",
      "410         NaN  NaN       NaN\n",
      "411         NaN  NaN       NaN\n",
      "412         NaN  NaN       NaN\n",
      "413         NaN  NaN       NaN\n",
      "414         NaN  NaN       NaN\n",
      "415         NaN  NaN       NaN\n",
      "416         NaN  NaN       NaN\n",
      "417         NaN  NaN       NaN\n",
      "418         NaN  NaN       NaN\n",
      "419         NaN  NaN       NaN\n",
      "420         NaN  NaN       NaN\n",
      "421         NaN  NaN       NaN\n",
      "422         NaN  NaN       NaN\n",
      "423         NaN  NaN       NaN\n",
      "424         NaN  NaN       NaN\n",
      "425         NaN  NaN       NaN\n",
      "426         NaN  NaN       NaN\n",
      "427         NaN  NaN       NaN\n",
      "428         NaN  NaN       NaN\n",
      "429         NaN  NaN       NaN\n",
      "430         NaN  NaN       NaN\n",
      "431         NaN  NaN       NaN\n",
      "432         NaN  NaN       NaN\n",
      "433         NaN  NaN       NaN\n",
      "434         NaN  NaN       NaN\n",
      "435         NaN  NaN       NaN\n",
      "436         NaN  NaN       NaN\n",
      "437         NaN  NaN       NaN\n",
      "438         NaN  NaN       NaN\n",
      "439         NaN  NaN       NaN\n",
      "440         NaN  NaN       NaN\n",
      "441         NaN  NaN       NaN\n",
      "442         NaN  NaN       NaN\n",
      "443         NaN  NaN       NaN\n",
      "444         NaN  NaN       NaN\n",
      "445         NaN  NaN       NaN\n",
      "446         NaN  NaN       NaN\n",
      "447         NaN  NaN       NaN\n",
      "448         NaN  NaN       NaN\n",
      "449         NaN  NaN       NaN\n",
      "450         NaN  NaN       NaN\n",
      "451         NaN  NaN       NaN\n",
      "452         NaN  NaN       NaN\n",
      "453         NaN  NaN       NaN\n",
      "454         NaN  NaN       NaN\n",
      "455         NaN  NaN       NaN\n",
      "456         NaN  NaN       NaN\n",
      "457         NaN  NaN       NaN\n",
      "458         NaN  NaN       NaN\n",
      "459         NaN  NaN       NaN\n",
      "460         NaN  NaN       NaN\n",
      "461         NaN  NaN       NaN\n",
      "462         NaN  NaN       NaN\n",
      "463         NaN  NaN       NaN\n",
      "464         NaN  NaN       NaN\n",
      "465         NaN  NaN       NaN\n",
      "466         NaN  NaN       NaN\n",
      "467         NaN  NaN       NaN\n",
      "468         NaN  NaN       NaN\n",
      "469         NaN  NaN       NaN\n",
      "470         NaN  NaN       NaN\n",
      "471         NaN  NaN       NaN\n",
      "472         NaN  NaN       NaN\n",
      "473         NaN  NaN       NaN\n",
      "474         NaN  NaN       NaN\n",
      "475         NaN  NaN       NaN\n",
      "476         NaN  NaN       NaN\n",
      "477         NaN  NaN       NaN\n",
      "478         NaN  NaN       NaN\n",
      "479         NaN  NaN       NaN\n",
      "480         NaN  NaN       NaN\n",
      "481         NaN  NaN       NaN\n",
      "482         NaN  NaN       NaN\n",
      "483         NaN  NaN       NaN\n",
      "484         NaN  NaN       NaN\n",
      "485         NaN  NaN       NaN\n",
      "486         NaN  NaN       NaN\n",
      "487         NaN  NaN       NaN\n",
      "488         NaN  NaN       NaN\n",
      "489         NaN  NaN       NaN\n",
      "490         NaN  NaN       NaN\n",
      "491         NaN  NaN       NaN\n",
      "492         NaN  NaN       NaN\n",
      "493         NaN  NaN       NaN\n",
      "494         NaN  NaN       NaN\n",
      "495         NaN  NaN       NaN\n",
      "496         NaN  NaN       NaN\n",
      "497         NaN  NaN       NaN\n",
      "498         NaN  NaN       NaN\n",
      "499         NaN  NaN       NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/granch_utils/init_model_tensor.py:280: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.1458208013756522' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.all_observations.loc[self.current_t]  = Normal(current_stimulus, noise_epsilon).sample().tolist()\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(main_sim_tensor)\n",
    "importlib.reload(lesioned_sim)\n",
    "importlib.reload(init_params_tensor)\n",
    "importlib.reload(compute_prob_tensor)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "BATCH_INFO = {\n",
    "    \"jitter_n\": 1, \n",
    "    \"total_batch_n\": 1, \n",
    "    \"jitter_mode\": \"sampling\"\n",
    "}\n",
    "\n",
    "GRID_INFO = {\n",
    "    \"grid_mu_start\": -4, \"grid_mu_end\": 4, \"grid_mu_step\": 20, \n",
    "    \"grid_sigma_start\": 0.001, \"grid_sigma_end\": 1.8, \"grid_sigma_step\": 20, \n",
    "    \"grid_y_start\": -4, \"grid_y_end\": 4, \"grid_y_step\": 20, \n",
    "    \"grid_epsilon_start\": 0.000001, \"grid_epsilon_end\": 1, \"grid_epsilon_step\": 20, \n",
    "    \"hypothetical_obs_grid_n\": 10\n",
    "}\n",
    "\n",
    "\n",
    "BATCH_GRID_INFO = num_stab_help.get_batch_grid(BATCH_INFO, GRID_INFO)\n",
    "\n",
    "PRIOR_INFO = {\n",
    "    \"mu_prior\": 0,  \n",
    "    \"V_prior\": 1, \n",
    "    \"alpha_prior\": 3, \n",
    "    \"beta_prior\": 1, \n",
    "    \"epsilon\": 0.0001, \"mu_epsilon\":0.0001, \"sd_epsilon\": 0.0001, \n",
    "    \"hypothetical_obs_grid_n\": 10, \n",
    "    \"world_EIGs\": 0.0001, \"max_observation\": 500\n",
    "}\n",
    "\n",
    "tensor_stimuli = num_stab_help.sample_spore_experiment(pair_each_stim = 1, n_feature=1)\n",
    "\n",
    "tensor_model =  init_model_tensor.granch_model(PRIOR_INFO['max_observation'], tensor_stimuli[0])\n",
    "\n",
    "params = init_params_tensor.granch_params(\n",
    "                grid_mu =  BATCH_GRID_INFO[\"grid_mus\"][0].to(device),\n",
    "                grid_sigma = BATCH_GRID_INFO[\"grid_sigmas\"][0].to(device),\n",
    "                grid_y = BATCH_GRID_INFO[\"grid_ys\"][0].to(device),\n",
    "                grid_epsilon = BATCH_GRID_INFO[\"grid_epsilons\"][0].to(device),\n",
    "                hypothetical_obs_grid_n = PRIOR_INFO[\"hypothetical_obs_grid_n\"], \n",
    "                mu_prior = PRIOR_INFO[\"mu_prior\"],\n",
    "                V_prior = PRIOR_INFO[\"V_prior\"], \n",
    "                alpha_prior = PRIOR_INFO[\"alpha_prior\"], \n",
    "                beta_prior = PRIOR_INFO[\"beta_prior\"],\n",
    "                epsilon  = PRIOR_INFO[\"epsilon\"], \n",
    "                mu_epsilon = PRIOR_INFO[\"mu_epsilon\"], \n",
    "                sd_epsilon = PRIOR_INFO[\"sd_epsilon\"], \n",
    "                world_EIGs = PRIOR_INFO[\"world_EIGs\"],\n",
    "                max_observation = PRIOR_INFO[\"max_observation\"], \n",
    "                forced_exposure_max= np.nan)\n",
    "        \n",
    "            # add the various different cached bits\n",
    "params.add_meshed_grid()\n",
    "params.add_lp_mu_sigma()\n",
    "params.add_y_given_mu_sigma()\n",
    "params.add_lp_epsilon()\n",
    "params.add_priors()\n",
    "\n",
    "res = main_sim_tensor.granch_main_simulation(params, tensor_model, tensor_stimuli[0])\n",
    "#res = lesioned_sim.granch_no_learning_simulation(params, tensor_model, tensor_stimuli[0])\n",
    "#res = lesioned_sim.granch_no_noise_simulation(params, tensor_model, tensor_stimuli[0])\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(res.behavior)\n",
    "#main_sim_tensor.granch_main_simulation(PRIOR_INFO, tensor_model, tensor_stimuli)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([0.2030], dtype=torch.float64),\n",
       " 1: tensor([0.2030], dtype=torch.float64),\n",
       " 2: tensor([0.2030], dtype=torch.float64),\n",
       " 3: tensor([0.2030], dtype=torch.float64),\n",
       " 4: tensor([0.2030], dtype=torch.float64),\n",
       " 5: tensor([0.2030], dtype=torch.float64)}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stim =  tensor_stimuli[0].stimuli_sequence\n",
    "test_stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, indices \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(indices_combinations):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m col, row \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(indices):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         expanded_tensor[idx, col] \u001b[39m=\u001b[39m original_tensor[row, col]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Print the expanded tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(expanded_tensor)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensors A and B\n",
    "A = torch.tensor([1, 2, 3])\n",
    "B = torch.tensor([4, 5, 6])\n",
    "C = torch.tensor([7, 8, 9])\n",
    "\n",
    "# Create all combinations using torch.meshgrid()\n",
    "grid_A, grid_B, grid_C= torch.meshgrid(A,B,C)\n",
    "\n",
    "# Stack the grids to form tensor C\n",
    "D = torch.stack((grid_A, grid_B, grid_C), dim=-1)\n",
    "\n",
    "#print(D)\n",
    "\n",
    "# Reshape tensor C to 2D if needed\n",
    "D = D.view(-1, 3)\n",
    "\n",
    "# Print the resulting tensor C\n",
    "print(D.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.meshgrid: Expected 0D or 1D tensor in the tensor list but got:  3  5\n 3  6\n 4  5\n 4  6\n[ torch.LongTensor{4,2} ]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m B \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m C \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m5\u001b[39m, \u001b[39m6\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m result \u001b[39m=\u001b[39m combine_tensors(A, B, C)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Print the resulting tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "\u001b[1;32m/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m left \u001b[39m=\u001b[39m combine_tensors(\u001b[39m*\u001b[39mtensors[:half])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m right \u001b[39m=\u001b[39m combine_tensors(\u001b[39m*\u001b[39mtensors[half:])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m combine_tensors(left, right)\n",
      "\u001b[1;32m/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tensors[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(tensors) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Combine two tensors using broadcasting\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(torch\u001b[39m.\u001b[39;49mmeshgrid(\u001b[39m*\u001b[39;49mtensors), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Recursively combine N tensors by reducing them to two at a time\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caoanjie/Desktop/projects/looking_time_models/02_pyGRANCH/mini.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     half \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tensors) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/functional.py:489\u001b[0m, in \u001b[0;36mmeshgrid\u001b[0;34m(indexing, *tensors)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmeshgrid\u001b[39m(\u001b[39m*\u001b[39mtensors, indexing: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[1;32m    397\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Creates grids of coordinates specified by the 1D inputs in `attr`:tensors.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m \u001b[39m    This is helpful when you want to visualize data over some\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \n\u001b[1;32m    488\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mreturn\u001b[39;00m _meshgrid(\u001b[39m*\u001b[39;49mtensors, indexing\u001b[39m=\u001b[39;49mindexing)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/torch/functional.py:504\u001b[0m, in \u001b[0;36m_meshgrid\u001b[0;34m(indexing, *tensors)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39m# Continue allowing call of old method that takes no indexing\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m# kwarg for forward compatibility reasons.\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# Remove this two weeks after landing.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m kwargs \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m indexing \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mindexing\u001b[39m\u001b[39m'\u001b[39m: indexing}\n\u001b[0;32m--> 504\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mmeshgrid(tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.meshgrid: Expected 0D or 1D tensor in the tensor list but got:  3  5\n 3  6\n 4  5\n 4  6\n[ torch.LongTensor{4,2} ]"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
