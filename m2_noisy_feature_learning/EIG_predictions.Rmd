---
title: "EIG computation"
author: "anjie & mike"
date: "6/3/2021"
output: html_document
---

# Preliminaries

```{r}
# library(reshape2)
library(tidyverse)
library(here)
library(matrixStats)
```


```{r}
## source relevant files
source(here("helper/get_entropy.R"))
source(here("helper/get_KL_measurement.R"))
source(here("helper/get_surprise.R"))
source(here("helper/noisy_update.R"))
source(here("helper/grid_approximation.R"))
source(here("helper/get_stimuli_and_observations.R"))
```

# Generate scenario

```{r}
num_features = 1
num_features_simple = 1
# num_features_complex = 8
trials_per_block = 8
deviant_positions = c(3,5)
dissimilarity_ratio = 0.2

# must satisfy: total feature > (1 + dissimilar ratio) * featureOnumber
simple_stimuli <- generate_creature_sequence(
  block_length = trials_per_block, 
  deviant_positions = deviant_positions,  # takes a vector, 
  total_feature = num_features, 
  feature_theta = 0.8, 
  feature_number = num_features_simple, 
  dissimilar_ratio = dissimilarity_ratio)


simple_observations <- 
  generate_noisy_observations(block = simple_stimuli, 
                              exposure_type = "self_paced", 
                              short_exposure_samps = 1, 
                              long_exposure_samps = 10, 
                              normal_exposure_samps = 10, 
                              epsilon = 0.02)    
```

# Posterior inference

First start by computing the posterior after observation 1. 

```{r}
grid_theta <- seq(0.1, 1, 0.2)
grid_epsilon <- seq(0.1, 1, 0.2)
alpha_prior = 1
beta_prior = 1
alpha_epsilon = 1 
beta_epsilon = 10

obs_matrix <- simple_observations %>% 
    ungroup() %>%
    select(-c(trial_num, observation_num)) %>% 
    as.matrix()

posterior_o1 <- grid_with_theta_and_epsilon(grid_theta = grid_theta, 
                                            grid_epsilon = grid_epsilon, 
                                            noisy_observation = obs_matrix[1,], 
                                            alpha_prior = alpha_prior, 
                                            beta_prior= beta_prior, 
                                            alpha_epsilon = alpha_epsilon, 
                                            beta_epsilon = beta_epsilon)
```

Now we want to choose what to do. Do we keep looking at stimulus 1 or do we look away? 

To figure this out, we need to choose proportional to expected information gain vs. the constant information gain from looking away.  

To do that, we compute EIG. 

- calculate posterior predictive distribution:
    - compute p(z|theta, epsilon) --> will give P(z_ij=0) and P(z_ij=1)
- get EIG with entropy:
if true z = 1: P(z_ij=0) * (previous - alternative entropy) + P(z_ij=1) * (previous - entropy)
else if z = 0: P(z_ij=0) * (previous - entropy) + P(z_ij=1) * (previous - alternative entropy)
- get EIG with KL:
if true z = 1: P(z_ij=0) * KL(previous dist||alternative dist) + P(z_ij=1) * KL(previous dist||current dist)
else if z = 0: P(z_ij=0) * KL(previous dist||current dist) + P(z_ij=1) * KL(previous dist||alternative dist)

Let's do KL divergence for starters. So we want to compute D_{KL} for t2 under the two possible scenarios, and then weight these by the posterior predictive probability of the two scenarios. 

```{r}
o2_s1 <- obs_matrix
o2_s1[2,1] <- 0
o2_s2 <- obs_matrix
o2_s2[2,1] <- 1

posterior_o2_s1 <- grid_with_theta_and_epsilon(grid_theta = grid_theta, 
                                               grid_epsilon = grid_epsilon, 
                                               noisy_observation = head(o2_s1,2), 
                                               alpha_prior = alpha_prior, 
                                               beta_prior= beta_prior, 
                                               alpha_epsilon = alpha_epsilon, 
                                               beta_epsilon = beta_epsilon)

posterior_o2_s2 <- grid_with_theta_and_epsilon(grid_theta = grid_theta, 
                                               grid_epsilon = grid_epsilon, 
                                               noisy_observation = head(o2_s2,2), 
                                               alpha_prior = alpha_prior, 
                                               beta_prior= beta_prior, 
                                               alpha_epsilon = alpha_epsilon, 
                                               beta_epsilon = beta_epsilon)
```

Now let's think through how to compute the KL and posterior predictive.

Here's a simple version where we pretend epsilon = 0. 

```{r}
# this will have underflow issues for small numbers
dkl <- function (x,y) {
  sum(x * log(x / y)) 
}

# very simple posterior predictive
post_pred <- function(theta, p_theta, heads = TRUE) {
   ifelse(heads, 
          sum(theta * p_theta), 
          1 - sum(theta * p_theta))
}

dkl_s1 <- dkl(posterior_o2_s1$posterior, posterior_o1$posterior)
dkl_s2 <- dkl(posterior_o2_s2$posterior, posterior_o1$posterior)

post_pred_s1_0 <- post_pred(posterior_o1$theta, posterior_o1$posterior, heads = FALSE)
post_pred_s1_1 <- post_pred(posterior_o1$theta, posterior_o1$posterior, heads = TRUE)

EIG = post_pred_s1_0 * dkl_s1 + post_pred_s1_1 * dkl_s2
```

Let's make it a little more complicated and compute it when we keep track of the full posterior over epsilon and theta. 

Now we will want to compute the probability (based on theta AND epsilon) that:
y = 1, z = 1
y = 1, z = 0
y = 0, z = 1, 
y = 0, z = 0

BUT we will want to sum the probability across the values of Y because all we care about is the prediction about z!

```{r}

```

now p-lookaway is going to be a luce choice rule of 

$$p(keep looking) = \frac{EIG^\lambda}{(EIG^\lambda + C^\lambda)}$$

