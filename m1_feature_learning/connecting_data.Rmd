---
title: "connecting_data_model"
author: "anjie"
date: "4/6/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)

source(here("adult_modeling/scripts/archive/00_generate_stimuli.R"))
source(here("adult_modeling/scripts/archive/01_get_learning_measure.R"))
source(here("adult_modeling/scripts/archive/02_get_experiment_parameter.R"))
d <- read_csv(here("adult_modeling/data/processed_RTdata.csv"))
```

## recover the block sequence 
```{r}
# translate the experiment sequence into model-compatible structure 
d_experiment_parameter <- get_experiment_parameter(d)
```

recover the experiment sequence 
```{r}
d_exp_sim <- d_experiment_parameter %>% 
  # the block length doesn't match 
  mutate(
    
    sequence = pmap(d_experiment_parameter %>% select(-c(subject, block_number)), .f = ~with(list(...), 
                                             get_block_sequence(complexity, similarity, 
                                                                20, 
                                                                5, 10, 
                                                                0.2, 0.8, 
                                                                block_length, 
                                                                dev_positions)))
  ) %>% 
   mutate(beta = map(.x = sequence, 
                    .f = get_beta_count), 
         probability = map(.x = beta, 
                           .f = get_probability), 
         surprise = map2(.x = probability, 
                         .y = sequence, 
                         .f = get_surprise), 
         learning_progress = map(
           .x = probability, 
           .f = get_learning_progress
         )) %>% 
  unnest(sequence, surprise, learning_progress) %>%
  unnest(learning_progress) %>% 
  # figure out the corresponding trial number 
  group_by(subject, block_number) %>% 
  mutate(trial_number = row_number()) 
```




```{r}
d_rt <- d %>% 
  select(subject, block_number, trial_number, rt, item_type, trial_type, trial_complexity) %>% 
  mutate(rt = rt + 500) %>%  # add the baseline back 
  mutate(temp_id = paste(subject, block_number, trial_number)) %>% 
  #rename(real_trial_number = trial_number) %>% 
  select(temp_id, rt, item_type, trial_type, trial_complexity)

d_exp_sim <- d_exp_sim %>% 
  mutate(temp_id = paste(subject, block_number, trial_number)) %>% 
  left_join(d_rt, by = "temp_id") 


```

now d_exp_sim has all the relevant info, let's plot them 


```{r}
d_exp_sim %>% 
  pivot_longer(cols = c(surprise, learning_progress, rt), 
               names_to = "measure_type", 
               values_to = "measure_value") %>% 
  filter(measure_type == "surprise") %>% 
  ggplot(
       aes(x=trial_number, y=measure_value, colour=item_type)) + 
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) + 
  geom_smooth(method = "lm", 
              formula = y ~ I(exp(1)**(-x)), se = FALSE) + 
  facet_grid(~trial_complexity) +
  langcog::scale_color_solarized(name = "Item Type") + 
  ylab("surprise") + 
  xlab("Trial Number") +
   theme_classic()+ 
   theme(
    axis.text=element_text(size=12),
    axis.title=element_text(size=14,face="bold"), 
     legend.position = "bottom") 


d_exp_sim %>% 
  pivot_longer(cols = c(surprise, learning_progress, rt), 
               names_to = "measure_type", 
               values_to = "measure_value") %>% 
  filter(measure_type == "learning_progress") %>% 
  ggplot(
       aes(x=trial_number, y=measure_value, colour=item_type)) + 
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) + 
  geom_smooth(method = "lm", 
              formula = y ~ I(exp(1)**(-x)), se = FALSE) + 
  facet_grid(~trial_complexity) +
  langcog::scale_color_solarized(name = "Item Type") + 
  theme(legend.position = "bottom") + 
  ylab("learning_progress") + 
  xlab("Trial Number") + 
  theme_classic()+
  theme(
    axis.text=element_text(size=12),
    axis.title=element_text(size=14,face="bold"), 
     legend.position = "bottom") 


d_exp_sim %>% 
  pivot_longer(cols = c(surprise, learning_progress, rt), 
               names_to = "measure_type", 
               values_to = "measure_value") %>% 
  filter(measure_type == "rt") %>% 
  ggplot(
       aes(x=trial_number, y=log(measure_value), colour=item_type)) + 
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) + 
  geom_smooth(method = "lm", 
              formula = y ~ I(exp(1)**(-x)), se = FALSE) + 
  facet_grid(~trial_complexity) +
  langcog::scale_color_solarized(name = "Item Type") + 
  ylab("Log looking time (ms)") + 
  xlab("Trial Number")  + 
  theme_classic()+
  theme(
    axis.text=element_text(size=12),
    axis.title=element_text(size=14,face="bold"), 
    legend.position = "bottom")
  
```


pivot_longer(cols = c(mean_rt, mean_kl, mean_surprise), 
               names_to = "measure_type", 
               values_to = "measure_value") %>% 
  ggplot(aes(x = measure_value,))
  
```{r}

d_exp_sim %>% 
  group_by(trial_number, item_type, trial_complexity) %>% 
  summarise(
    mean_rt = mean(log(rt)), 
    mean_kl = mean(learning_progress), 
    mean_surprise = mean(surprise)
  ) %>% 
  ggplot(aes(x= mean_kl, y = mean_rt)) +
  geom_point() + 
  geom_smooth(method = "lm") + 
  ylab("log looking time (ms)") + 
  xlab("KL divergence") + 
  theme_classic() + 
  theme(
    axis.text=element_text(size=12),
    axis.title=element_text(size=14,face="bold"))
  
```

```{r}
d_exp_sim %>% 
  group_by(trial_number, item_type, trial_complexity) %>% 
  summarise(
    mean_rt = mean(log(rt)), 
    mean_kl = mean(learning_progress), 
    mean_surprise = mean(surprise)
  ) %>% 
  ggplot(aes(x= mean_surprise, y = mean_rt)) +
  geom_point() + 
  geom_smooth(method = "lm") + 
  ylab("log looking time (ms)") + 
  xlab("Surprise") + 
  theme_classic() + 
  theme(
    axis.text=element_text(size=12),
    axis.title=element_text(size=14,face="bold"))
```




# explore correlation 

```{r}
d_cor <- d_exp_sim %>% 
  select(subject, block_number, trial_number, 
         trial_type, trial_complexity, item_type,
         surprise, learning_progress, rt) %>% 
  mutate(log_rt = log(rt))

d_cor_summary <- d_cor %>% 
  ungroup() %>% 
  group_by(item_type, 
           trial_complexity, 
           trial_number) %>% 
  summarise(
    mean_suprirse = mean(surprise), 
    mean_lp = mean(learning_progress),
    mean_log_rt = mean(log_rt))
```


```{r}
d_cor_summary
```

## surprise and rt 
```{r}

d_cor_summary %>% 
  ggplot(aes(x = mean_suprirse, y = log(mean_log_rt))) + 
  geom_jitter(height = .1, alpha = .1) + 
  geom_smooth()

d_cor_summary %>% 
  ggplot(aes(x = mean_suprirse, y = log(mean_log_rt), color = trial_complexity)) + 
  geom_jitter(height = .1, alpha = .1) + 
  geom_smooth()

d_cor_summary %>% 
  ggplot(aes(x = mean_suprirse, y = log(mean_log_rt), color = item_type)) + 
  geom_jitter(height = .1, alpha = .1) + 
  geom_smooth()


```

## learning progress and rt 
```{r}

d_cor_summary %>% 
  ggplot(aes(x = mean_lp, y = log(mean_log_rt))) + 
  geom_jitter(height = .1, alpha = .1) + 
  geom_smooth()

d_cor_summary %>% 
  ggplot(aes(x = mean_lp, y = log(mean_log_rt), color = trial_complexity)) + 
  geom_jitter(height = .1, alpha = .1) + 
  geom_smooth()

d_cor_summary %>% 
  ggplot(aes(x = mean_lp, y = log(mean_log_rt), color = item_type)) + 
  geom_jitter(height = .1, alpha = .1) + 
  geom_smooth()
```

## omg why is it so flat learning progress and surprise? 
```{r}
d_cor_summary %>% 
  ggplot(aes(x = mean_suprirse, y = mean_lp)) + 
  geom_jitter(height = .1, alpha = .1) + 
  geom_smooth()

d_cor_summary %>% 
  ggplot(aes(x = mean_suprirse, y = mean_lp, color = trial_complexity)) + 
  geom_jitter(height = .1, alpha = .1) + 
  geom_smooth()

d_cor_summary %>% 
  ggplot(aes(x = mean_suprirse, y = mean_lp, color = item_type)) + 
  geom_jitter(height = .1, alpha = .1) + 
  geom_smooth()
```


# Calculate RMSE and Pearson 
```{r}
library(Metrics)
rmse(d_cor$log_rt, d_cor$learning_progress)
cor(x = d_cor$log_rt, y = d_cor$learning_progress, method = "pearson")
rmse(d_cor$log_rt, d_cor$surprise)
cor(x = d_cor$log_rt, y = d_cor$surprise, method = "pearson")
```

```{r}
cor(x = d_cor_summary$mean_log_rt, y = d_cor_summary$mean_suprirse, method = "pearson")

cor(x = d_cor_summary$mean_log_rt, y = d_cor_summary$mean_lp, method = "pearson")

```


## now try a bunch of parameters 

```{r}
#technically we only need one sequence if we want to vary prior


df_sequence <- generate_sequence_with_parameter(
  d_experiment_parameter, 
                   200, 
                   100, 
                   50, 
                   0.2, 
                   0.6)


```


```{r}
df_sequence
```

## do a line by line profiling 
```{r}
library(lineprof)
library(furrr)
test_beta_count <- function(df_sequence){
  get_beta_count <- function(block, feature_prior = c(1, 1)){
    prior <- replicate(length(block[[1]]), feature_prior, simplify = FALSE)
    
    beta_count <- list()
    beta_count[[1]] <- prior 
    for (trial in 1:length(block)){
      beta_count[[trial+1]] <- mapply(function(x, y) {
           x[y + 1] <- x[y + 1] + 1
           return(list(x))
         },
         beta_count[[trial]], 
         block[[trial]])
    }
  
      return(beta_count)

  }
  
  
  d_measure <- df_sequence %>% 
   mutate(beta = future_map(.x = sequence, 
                    .f = get_beta_count)) 
  
  return(d_measure)
  
}

ptm <- proc.time()
test <- test_beta_count(df_sequence)
proc.time() - ptm

library(foreach)
library(doParallel)
myCluster <- makeCluster(7, TYPE = "PSOCK")
registerDoParallel(myCluster)


test_beta_count <- function(df_sequence){
  
  
  get_beta_count <- function(block, feature_prior = c(1, 1)){
    prior <- replicate(length(block[[1]]), feature_prior, simplify = FALSE)
    
    beta_count <- list()
    beta_count[[1]] <- prior 
    for (trial in 1:length(block)){
      beta_count[[trial+1]] <- mapply(function(x, y) {
           x[y + 1] <- x[y + 1] + 1
           return(list(x))
         },
         beta_count[[trial]], 
         block[[trial]])
    }
    
      return(beta_count)

  }
  
  get_beta_count_parallel <- function(block, feature_prior = c(1, 1)){
    
    prior <- replicate(length(block[[1]]), feature_prior, simplify = FALSE)
    
    beta_count <- list()
    beta_count[[1]] <- prior 
    foreach (trial = 1:length(block)) %dopar% {
      beta_count[[trial+1]] <- mapply(function(x, y) {
           x[y + 1] <- x[y + 1] + 1
           return(list(x))
         },
         beta_count[[trial]], 
         block[[trial]])
    }
    
      return(beta_count)

  }
 
      
  ptm <- proc.time()
  d_measure <- df_sequence %>% 
   mutate(beta = map(.x = sequence, 
                    .f = get_beta_count_parallel), 
          # probability = future_map(.x = beta, 
          #                  .f = get_probability), 
          # surprise = future_map2(.x = probability, 
          #                .y = sequence, 
          #                .f = get_surprise), 
          # learning_progress = future_map(
          #  .x = probability, 
          #  .f = get_learning_progress)
          ) 
  time <- proc.time() - ptm
  
  return(time)
  
}

ptm <- proc.time()
test <- test_beta_count(df_sequence)
proc.time() - ptm
#test <- lineprof(test_beta_count(df_sequence))
```

time record 
just beta: 
user  system elapsed 
7.596   0.076   7.692 

beta + probability 
user  system elapsed 
13.047   0.245  13.384

beta + probability + surprise 
user  system elapsed 
17.808   0.685  18.539 

beta + probability + surprise + learning progress 
user  system elapsed 
24.679   0.476  25.209 
 

```{r}
library(lineprof)
calculate_prior <- function(d_sequence,  
                               prior) {
  
  
  get_beta_count <- function(block, feature_prior = prior){
    prior <- replicate(length(block[[1]]), feature_prior, simplify = FALSE)
    
    beta_count <- list()
    beta_count[[1]] <- prior 
    for (trial in 1:length(block)){
      beta_count[[trial+1]] <- mapply(function(x, y) {
           x[y + 1] <- x[y + 1] + 1
           return(list(x))
         },
         beta_count[[trial]], 
         block[[trial]])
    }
  
      return(beta_count)

  }
  
  
  d_measure <- d_sequence %>% 
   mutate(beta = map(.x = sequence, 
                    .f = get_beta_count), 
         probability = map(.x = beta, 
                           .f = get_probability), 
         surprise = map2(.x = probability, 
                         .y = sequence, 
                         .f = get_surprise), 
         learning_progress = map(
           .x = probability, 
           .f = get_learning_progress
         )) %>% 
  unnest(sequence, surprise, learning_progress) %>%
  unnest(learning_progress) #%>% 
  # figure out the corresponding trial number 
  #mutate( 
   #      prior =  lapply(list(prior), function(x) x/sum(x))
  #       ) 

 
  
  
  return(d_measure)
  
  
}

#ptm <- proc.time()
#df_calculated <- calculate_prior(df_sequence, c(3,1))
#proc.time() - ptm

df_calculate <- lineprof(calculate_prior(df_sequence, c(3,1)))

```

```{r}
df_calculate
```


```{r}
df_calculated
```

```{r}
combine_rt_df <- function(df_measurement, d_rt){
   d_rt <- d_rt %>% 
  select(subject, block_number, trial_number, rt, item_type, trial_type, trial_complexity) %>% 
   mutate(rt = rt + 500) %>%  # add the baseline back 
   mutate(temp_id = paste(subject, block_number, trial_number)) %>% 
   select(temp_id, rt, item_type, trial_type, trial_complexity)
   
   df_combined <- df_measurement %>% 
     group_by(subject, block_number) %>% 
     mutate(trial_number = row_number()) %>% 
    mutate(temp_id = paste(subject, block_number, trial_number)) %>% 
     left_join(d_rt, by = "temp_id") 
   
   return(df_combined)
  
}

df_combined <- combine_rt_df(df_calculated, d)

```

```{r}
df_combined
```



```{r}

calculate_correlation <- function(df){
  
  d <- df %>% 
    mutate(log_rt = log(rt))
  
  lp_rmse <- rmse(d$log_rt, d$learning_progress)
  s_rmse <- rmse(d$log_rt, d$surprise)
  
  lp_pearson <- cor(x = d$log_rt, y = d$learning_progress, 
                    method = "pearson")
  s_pearson <- cor(x = d$log_rt, y = d$surprise, 
                    method = "pearson")

  d_result <- d %>% 
    ungroup() %>% 
    mutate(
      lp_rmse = lp_rmse, 
      s_rmse = s_rmse, 
      lp_pearson = lp_pearson, 
      s_pearson = s_pearson) %>% 
    distinct(lp_rmse, s_rmse, lp_pearson, s_pearson)
    # ) %>% 
    # pivot_longer(
    #   lp_rmse:s_rmse, 
    #   names_to = "rmse_type", 
    #   values_to = "rmse_value"
    # ) %>% 
    # pivot_longer(
    #   lp_pearson:s_pearson, 
    #   names_to = "pearson_type", 
    #   values_to = "pearson_value"
    # )
  
  return (d_result)
}

calculate_correlation(df_combined)
```

## varying prior? 
```{r}
pos <- as.list(seq(1,5, 1))
neg <- as.list(seq(1, 5, 1))

#pos <- seq(1,10, 1)
#neg <- seq(1, 10, 1)

prior_df <- expand.grid(pos, neg) %>% 
  as.data.frame() %>% 
  unnest(`Var1`, `Var2`) %>% 
  filter(`Var1` > `Var2`)

 
#prior_list <- lapply(apply(prior_df,2 , identity), unlist)

prior_list <- list(c(1, 1), 
                   c(2, 1), 
                   c(3, 1), 
                   c(4, 1), 
                   c(5, 1),
                   c(6, 1),
                   c(3, 2), 
                   c(5, 2))

```

# looping over the prior list and run simulation
```{r}
correlation = list()


for (s in 1:length(prior_list)) {
    prior = prior_list[[s]]
    prior_value = prior[[1]] / (prior[[1]] + prior[[2]])

    sim_data <- calculate_prior(df_sequence, prior)
    combined_rt_df <- combine_rt_df(sim_data, d)
    sim_corrleation <- calculate_correlation(combined_rt_df) %>% 
      mutate(sim_id = s, 
             prior_v = prior_value)
    correlation[[s]] <- sim_corrleation  # add it to your list
}


calculated_correlation = do.call(rbind, correlation)
```

```{r}
calculated_correlation
save(calculated_correlation, file = here("adult_modeling/calculated_correlation.RData"))

```
```{r}
calculated_correlation %>% 
  pivot_lo
```





## a mini version 
```{r}
df_mini_sequence <- generate_sequence(d_experiment_parameter, 
                           d,
                   30, 
                   10, 
                   5, 
                   0.2, 
                   0.6)

correlation = list()


for (s in 1:length(prior_list)) {
    prior = prior_list[[s]]
    prior_value = prior[[1]] / (prior[[1]] + prior[[2]])

    sim_data <- calculate_prior(df_mini_sequence, prior)
    combined_rt_df <- combine_rt_df(sim_data, d)
    sim_corrleation <- calculate_correlation(combined_rt_df) %>% 
      mutate(sim_id = s, 
             prior_v = prior_value)
    correlation[[s]] <- sim_corrleation  # add it to your list
}


calculated_correlation = do.call(rbind, correlation)
```

```{r}
calculated_correlation %>% 
  pivot_longer(lp_rmse:s_pearson, names_to = "measure_type", 
               values_to = "correlation_value") %>% 
  separate(measure_type, into = c("learning_measure_type", "correlation_measure_type")) %>% 
  filter(correlation_measure_type == "rmse") %>% 
  ggplot(aes(x = prior_v, 
             y = correlation_value, 
             color = learning_measure_type)) + 
  geom_point() + 
  geom_line() + 
  ylab("rmse")

calculated_correlation %>% 
  pivot_longer(lp_rmse:s_pearson, names_to = "measure_type", 
               values_to = "correlation_value") %>% 
  separate(measure_type, into = c("learning_measure_type", "correlation_measure_type")) %>% 
  filter(correlation_measure_type == "pearson") %>% 
  ggplot(aes(x = prior_v, 
             y = correlation_value, 
             color = learning_measure_type)) + 
  geom_point() + 
  geom_line() + 
  ylab("pearson")
```

## trying different similar / dissimilar ratio 
```{r}
similarity_list = list(c(0.1, 0.3 ), 
                       c(0.1, 0.4 ), 
                       c(0.1, 0.5 ), 
                       c(0.1, 0.6 ), 
                       c(0.1, 0.7 ), 
                       c(0.2, 0.3 ), 
                       c(0.2, 0.4 ), 
                       c(0.2, 0.5 ), 
                       c(0.2, 0.6 ), 
                       c(0.2, 0.7 ))


correlation = list()


for (s in 1:length(similarity_list)) {
  
    df_mini_sequence <- generate_sequence(d_experiment_parameter, 
                           d,
                   30, 
                   10, 
                   5, 
                   similarity_list[[s]][1], 
                    similarity_list[[s]][2])

    
    prior = c(3, 1)

    sim_data <- calculate_prior(df_mini_sequence, c(3, 1))
    combined_rt_df <- combine_rt_df(sim_data, d)
    sim_corrleation <- calculate_correlation(combined_rt_df) %>% 
      mutate(sim_id = s, 
             prior_v = prior_value)
    correlation[[s]] <- sim_corrleation  # add it to your list
}


calculated_correlation = do.call(rbind, correlation)
```

```{r}
save(calculated_correlation, file = here("adult_modeling/varying_similarity.RData"))
calculated_correlation$similarity <- as.vector(similarity_list)
df_vis_correlation <- calculated_correlation %>% 
  unnest_wider(similarity) %>% 
  rename(similar_ratio = ...1, 
         dissimilar_ratio = ...2) %>% 
  mutate(sim_diff = dissimilar_ratio - similar_ratio, 
         sim_ratio = dissimilar_ratio / similar_ratio) %>% 
  pivot_longer(lp_rmse:s_pearson, names_to = "measure_type", 
               values_to = "correlation_value") %>% 
  pivot_longer(sim_diff : sim_ratio, 
               names_to = "similarity_measure", 
               values_to = "similarity_value") %>% 
  separate(measure_type, into = c("learning_measure_type", "correlation_measure_type")) 


  
  
```

```{r}
df_vis_correlation %>% 
filter(correlation_measure_type == "pearson") %>% 
  ggplot(aes(x = dissimilar_ratio, 
             y = correlation_value, 
             color = learning_measure_type)) + 
  geom_point() + 
  geom_line() + 
  ylab("pearson") + 
  facet_wrap(~similar_ratio)
```

