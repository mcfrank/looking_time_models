---
title: 'Quantifying the syntactic bootstrapping effect in verb learning: A meta-analytic
  synthesis'
author: "Anjie Cao and Molly Lewis"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: yes
    theme: paper
    toc: yes
    toc_float: no
  pdf_document:
    toc: yes
subtitle: Supplementary Information
---

******
******

```{r setup, include = F}
# load packages
library(tinytex)
library(tidyverse)
library(pwr)
library(knitr)
library(here)
library(kableExtra)
library(metafor)
library(ggimage)
library(imager)
library(stringr)
library(metapower)
library(glue)
library(heatmaply)
library(PublicationBias)
source(here("writeups/paper/scripts/model_print.R"))
source(here("writeups/paper/scripts/forest_plot_helper.R"))
source(here("writeups/paper/scripts/funnel_plot_helper.R"))
source(here("writeups/paper/scripts/predictor_plot_helper.R"))
source(here("writeups/paper/scripts/SI_table_helper.R"))
source(here("writeups/paper/scripts/SI_forest_helper.R"))
alpha = 0.5


opts_chunk$set(echo = F, message = F, warning = F, 
               error = F, cache = F, tidy = F, fig.height = 4.5)

theme_set(theme_classic())
options(shiny.sanitize.errors = FALSE)
```  

<a href="https://github.com/anjiecao/SyntacticBootstrappingMA" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm</style>

This document was created from an R markdown file. The repository for the project can be found [here](https://github.com/anjiecao/SyntacticBootstrapping). The data reported in the paper can be explored interactively at the [Metalab website](http://metalab.stanford.edu/).

```{r }
DATA_PATH <- here("data/processed/syntactic_bootstrapping_tidy_data.csv") 
RAW_DATA_PATH <- here("data/raw/syntactic_bootstrapping_raw_data.csv")

ma_data <- read_csv(DATA_PATH) 
ma_raw_data <- read_csv(RAW_DATA_PATH)


```

# Main models results using dataset without imputed values {.tabset}
For one paper missing relevant sufficient data to calculate an effect size (Hirsh-Pasek, Golinkoff, & Naigles, 1996), we imputed values from studies with similar design. The tables below report the model results from fitting the exact same models on the dataset excluding the imputed study. The two sets of models did not differ qualitatively. 


```{r}
MODERATORS <- c( "NULL", "mean_age_months","productive_vocab_median", "sentence_structure", "agent_argument_type", "patient_argument_type", "n_repetitions_sentence",  "stimuli_modality", "stimuli_actor", "transitive_event_type","intransitive_event_type", "presentation_type","character_identification", "practice_phase", "test_mass_or_distributed", "n_train_test_pair", "n_test_trial_per_pair" )

si_mod_print_no_impute <- generate_moderator_df(MODERATORS,filter(ma_data,  data_source != "imputed"))

```

## Predicate Type
```{r}
convert_pretty_print_table(si_mod_print_no_impute, "sentence_structure") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Sentence Structure" ~ "Predicate type (Intransitive / Transitive)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Noun phrase type
```{r}
convert_pretty_print_table(si_mod_print_no_impute, "agent_argument_type") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Agent Argument Type" ~ "Noun phrase type (Noun / Pronoun)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Character identification phase
```{r}
convert_pretty_print_table(si_mod_print_no_impute, "character_identification") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Character Identification" ~ "Character identification phase \n (No / Yes)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Practice phase 
```{r}
convert_pretty_print_table(si_mod_print_no_impute, "practice_phase") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Practice Phase" ~ "Practice phase \n (No / Yes)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Synchronicity 
```{r}
convert_pretty_print_table(si_mod_print_no_impute, "presentation_type") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Presentation Type" ~ "Synchronicity \n (Simultaneous / Asynchronous)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Testing structure 
```{r}
convert_pretty_print_table(si_mod_print_no_impute, "test_mass_or_distributed") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Test Mass Or Distributed" ~ "Testing Procedure Structure \n (Distributed / Mass)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```


## Number of sentence repetitions
```{r}
convert_pretty_print_table(si_mod_print_no_impute, "n_repetitions_sentence") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "N Repetitions Sentence" ~ "Number of sentence repetitions", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```



# Details of effect size calculation

Here we demonstrate our method for calculating effect sizes by describing a step-by-step  calculation for conditions in an example paper, Yuan & Fisher (2009). The table below shows the original data reported in the source paper (Table 1, pg 622 of Yuan & Fisher, 2009). The values are mean looking time in seconds (and corresponding SE).


```{r}
tibble(
  "Dialogue Type" = c("Transitive", "Intransitive"),
  "Sample Size" = c("8", "8"),
  "Two-participant Event" = c("4.82 (0.43)", "3.33 (0.24)"), 
  "One-participant Event" = c("2.87 (0.51)", "4.12 (0.40)")) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

To standardize the effect size calculation, we converted the reported raw results to the proportion of correct responses. For looking time studies, when the paper only reported the raw looking time in seconds, we calculated the proportion of correct response by dividing the mean looking time toward the matching scene by the sum of looking time toward the matching scenes and non-matching scenes (i.e., excluding the look away time from the denominator).  For children hearing transitive sentences, the correct scene was the two-participant event; for children hearing intransitive sentences, the correct scene was the one-participant event. Standard errors were converted using a similar method.



Using these standardize  we calculated Cohen's *d* and the variances as follows (the implementation of the script can be found at https://github.com/anjiecao/SyntacticBootstrappingMA/blob/master/analysis/scripts/02_calculate_es.R).

```{r, include = F}
tibble(
  "Dialogue Type" = c("Transitive", "Intransitive"),
    "Sample Size" = c("8", "8"),
  #"Mean Proportion Calculation" = c("4.82 / (4.82 + 2.87)", "4.12 / (3.33 + 4.12)"), 
  "Mean Proportion" = c("0.627", "0.553"), 
  #"Standard Deviation Calculation" = c("(0.43 / (4.82 + 2.87)) * (8^0.5)", "(0.4 / (3.33 + 4.12)) * (8^0.5)"),
  "Standard Deviation" = c("0.158", "0.152")) %>% kable() %>% 
  kable_styling(font_size = 12)
```



\begin{align}
Mean_{transitive} &= \frac{Time_{correct}}{Time_{correct} + Time_{incorrect}} \\
  &= \frac{4.82}{4.82 + 2.87} \\
  &= 0.627 \\
SD_{transitive} &= \frac{SE_{Raw}}{Time_{correct} + Time_{incorrect}} * \sqrt[2]{N} \\
  &= \frac{0.43}{4.82 + 2.87} *  \sqrt[2]{8} \\
  &= 0.158 \\
\end{align}



\begin{align}
Mean_{intransitive} &= \frac{Time_{correct}}{Time_{correct} + Time_{incorrect}} \\
  &= \frac{4.12}{3.33 + 4.12} \\
  &= 0.553 \\
SD_{intransitive} &= \frac{SE_{Raw}}{Time_{correct} + Time_{incorrect}} * \sqrt[2]{N} \\
  &= \frac{0.4}{3.33 + 4.12} *  \sqrt[2]{8} \\
  &= 0.152 \\
\end{align}



\begin{align}
 d_{transitive} &= \frac{M_1 - M_2}{\sigma_{pooled}} \\
  &= \frac{M_{correct} - M_{chance}}{\sigma_{correct}} \\
  &= \frac{0.627 - 0.5}{0.158} \\
  &\approx 0.79 \\
  d_{intransitive} &= \frac{M_1 - M_2}{\sigma_{pooled}} \\
  &= \frac{M_{correct} - M_{chance}}{\sigma_{correct}} \\
  &= \frac{0.553 - 0.5}{0.152} \\
  &\approx 0.35
\end{align}



\begin{align}
var(d_{transitive}) &= \frac{1}{N} + \frac{d^2}{2 * N} \\
&= \frac{1}{8} + \frac{0.79^2}{2 * 8} \\
&\approx 0.16 \\
var(d_{intransitive}) &= \frac{1}{N} + \frac{d^2}{2 * N} \\
&= \frac{1}{8} + \frac{0.35^2}{2 * 8} \\
&\approx 0.13 \\
\end{align}



# Comparison of between- vs. within- subject effect size estimates

The forest plot below compares the two ways of calculating effect sizes for the subset of experimental conditions that use a between-group analysis in the original paper.  In other words, the original analyses compared the proportion of looking time at the causative events between transitive conditions and intransitive conditions. Effect sizes calculated using this method are denoted by the pink points. We also present the effect sizes calculated using the against-chance method on the same subset of the experimental conditions. These effect sizes are denoted with green points. The against-chance method is a more conservative way of estimating the effect size. As the forest plot shows, the meta-analytic effect size using the between-group calculation is larger than the meta-analytic effect size using the against-chance method. 


```{r}
# fig.height= 10
# select the same subset of ES for comparison
alt_id <- alt_data %>% 
  filter(alternative_calc == "between") %>% 
  select(unique_id) %>% 
  pull

ma_subset <- ma_data %>% 
  filter(unique_id %in% alt_id) %>% 
  filter(sentence_structure == "transitive") %>% 
  select(unique_id, short_cite, 
         same_infant, plot_label, n_1, d_calc, d_var_calc, row_id) %>% 
  mutate(calc_type = "Chance Comparison")

alt_subset <- alt_data %>% 
  rowid_to_column() %>% 
  rename(row_id = rowid) %>% 
  select(unique_id, short_cite, 
         same_infant, plot_label,n_1, d_calc, d_var_calc, row_id) %>% 
  mutate(calc_type = "Cross-condition Comparison")

within_forest_data <- convert_to_forest_data(ma_subset)
between_forest_data <- convert_to_forest_data(alt_subset)
double_forest_data <- bind_rows(within_forest_data, between_forest_data)

generate_double_forest_plot(double_forest_data)



```



# Relationship between additional methodological moderators{.tabset}

We coded a number of additional methodological variables that substantially overlap with those in the paper. We report them here for completeness.

The additional coded moderators were as follows. First, we coded the modality of the visual stimuli. Stimuli modality has two levels: videos and animations. We coded this moderator following the details provided in the method sections of the papers. Stimuli actors have two levels, human actors and non-human actors. Studies using visual stimuli with human actors wearing animal suits were coded as using non-human actors. Second, we coded the types of the events presented in the visual stimuli. To capture the event types with details, we coded the transitive action stimuli and the intransitive action stimuli separately. For transitive action stimuli, we coded two levels: direct caused action and indirect caused action. The event was coded as using direct caused action if the agent in the action directly acted upon the patient. It was coded as using indirect caused action if the agent caused the patient to move via another medium. For example, the agent may pull a band on the patient's waist causing her to move. Likewise, the intransitive event also has two levels: one action versus parallel actions. Here we coded the levels by number of participants presented on the screen. An intransitive event was coded as "one action" if and only if there was only one agent presented on the screen. If an event involves more than one actor in the intransitive event (e.g. two actors doing parallel actions or one actor with one stander-by), then the event was coded as parallel-actions.

These additional moderators were not included in the main analyses because of their close relationships between each other and with the main moderators. The heatmaps below showed the overlap between moderators. Each cell corresponds to the co-occurrence between two moderator levels. Brighter colors indicate a higher frequency of co-occurrence, and darker colors indicate lower frequency. You can hover your mouse on the heatmap to see the corresponding value and combination of each cell. 


## Ordered by Row Average 
```{r fig.width=9.5, fig.height=9.5}
ALL_CATEGORICAL_VARS <- c("presentation_type",
                      #"agent_argument_type", "patient_argument_type", 
                     "stimuli_modality", "stimuli_actor", "character_identification",   "practice_phase", "test_mass_or_distributed", "transitive_event_type", "intransitive_event_type")


get_cross_counts <- function(args, df){
  var1 = args[[1]]
  var2 = args[[2]]
  
  if (var1 != var2){
  
  df %>%
    select_(var1, var2) %>%
    rename(v1 = var1, 
          v2 = var2) %>%
    count(v1, v2) %>%
    mutate(v1_long = glue("{var1}/{v1}"),
           v2_long = glue("{var2}/{v2}"))  %>%
    select(v1_long, v2_long, n) 

  }
}

all_pair_counts <- list(ALL_CATEGORICAL_VARS,
                        ALL_CATEGORICAL_VARS) %>%
  cross() %>%
  map_df(get_cross_counts, ma_data) %>%
  complete(v1_long, v2_long, fill = list(n = 0)) %>%
  filter(v1_long != v2_long)


  all_counts_wide <- all_pair_counts %>%
    pivot_wider(names_from = v2_long, values_from = n) 
  

  all_counts_wide_matrix <- all_counts_wide %>%
    select(-v1_long) %>%
    as.matrix()
  
  row.names(all_counts_wide_matrix) <-  as.character(all_counts_wide$v1_long)
  heatmaply(all_counts_wide_matrix,
            fontsize_row = 8,
            fontsize_col = 8)
```

## Ordered by groups        
```{r fig.width=9.5, fig.height=9.5}
ALL_VARS <- colnames(all_counts_wide_matrix)
ALL_VARS <- c("character_identification/yes", 
              "character_identification/no", 
              "practice_phase/yes", 
              "practice_phase/no", 
              "transitive_event_type/direct_caused_action",
              "transitive_event_type/indirect_caused_action", 
              "intransitive_event_type/one_action", 
              "intransitive_event_type/parallel_actions" , 
              "presentation_type/asynchronous", 
              "presentation_type/simultaneous", 
              "test_mass_or_distributed/mass", 
              "test_mass_or_distributed/distributed", 
              "stimuli_actor/person", 
              "stimuli_actor/non_person", 
              "stimuli_modality/video", 
              "stimuli_modality/animation"
              )

ordered_matrix <- all_counts_wide_matrix %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  slice(match(ALL_VARS, rowname)) %>% 
  column_to_rownames("rowname") %>% 
  select(all_of(ALL_VARS))

heatmaply(ordered_matrix,
            fontsize_row = 8,
            fontsize_col = 8,
           Rowv = NA, 
          Colv = NA)
```




# Sensitivity analysis

```{r}
ma_data_with_affirm <- ma_data %>%
  mutate(pvalue =  2 * (1 - pnorm( abs(d_calc / sqrt(d_var_calc)))),
         affirm =  (d_calc > 0) & (pvalue < 0.05))

affirm_model<- rma.mv(d_calc,  d_var_calc,  
                         random = ~ 1 | short_cite/same_infant/row_id, data=
          ma_data_with_affirm %>% filter(affirm == FALSE)) 

affirm_estimate <- as.numeric(affirm_model$b)

worst_case_estimate_print <- paste0(as.numeric(round(affirm_model$beta, 2)),
                                    " [",
                                    as.numeric(round(affirm_model$ci.lb, 2)),
                                    ", ",                                                                                                        as.numeric(round(affirm_model$ci.ub, 2)),
                                    "]")
 
all_model <- rma.mv(d_calc,  d_var_calc,  
                         random = ~ 1 | short_cite/same_infant/row_id, data=
          ma_data_with_affirm)
all_estimate <- as.numeric(all_model$ b)

```

The plot below shows a modified funnel plot, or "significance funnel" where significant studies are shown in orange and non-significant studies are shown in grey (Marthur & VanderWeele, 2020). The x-axis shows effect size estimates, and the y-axis shows estimated standard error for each estimate. Studies lying on the grey line have a p-value of .05. The black diamond shows the meta-analytic effect size estimate for all studies; the grey diamond shows the meta-analytic effect size estimate for significant studies only (the "worst-case" publication scenario). Note that the worst case scenario appreciable attenuates the effect size estimate, but does not attenuate the point estimate to 0 (worst case estimate: `r worst_case_estimate_print`).



```{r}

significance_funnel <- function (yi, vi, xmin = min(yi), xmax = max(yi), ymin = 0, ymax = max(sqrt(vi)), 
    xlab = "Point estimate", ylab = "Estimated standard error", 
    favor.positive = NA, est.all = NA, est.N = NA, alpha.select = 0.05, 
    plot.pooled = TRUE) 
{
    d = data.frame(yi, vi)
    d$sei = sqrt(vi)
    d$pval = 2 * (1 - pnorm(abs(yi)/sqrt(vi)))
    if (!is.na(est.all) & is.na(favor.positive)) {
        favor.positive = (est.all > 0)
        warning("favor.positive not provided, so assuming publication bias favors estimates whose sign matches est.all")
    }
    if (is.na(est.all) & is.na(favor.positive)) {
        stop("Need to specify favor.positive")
    }
    d$affirm = rep(NA, nrow(d))
    if (favor.positive == TRUE) {
        d$affirm[(d$yi > 0) & (d$pval < alpha.select)] = "Affirmative"
        d$affirm[(d$yi < 0) | (d$pval >= alpha.select)] = "Non-affirmative"
    }
    if (favor.positive == FALSE) {
        d$affirm[(d$yi < 0) & (d$pval < alpha.select)] = "Affirmative"
        d$affirm[(d$yi > 0) | (d$pval >= alpha.select)] = "Non-affirmative"
    }
    d$affirm = factor(d$affirm, c("Non-affirmative", "Affirmative"))
    if (sum(d$affirm == "Non-affirmative") == 0) {
        stop("There are no non-affirmative studies. The plot would look silly.")
    }
    if (sum(d$affirm == "Affirmative") == 0) {
        stop("There are no affirmative studies. The plot would look silly.")
    }
    if (is.na(est.N) & is.na(est.all)) {
        est.N = rma.uni(yi = d$yi[d$affirm == "Non-affirmative"], 
            vi = d$vi[d$affirm == "Non-affirmative"], method = "FE")$b
        est.all = rma.uni(yi = d$yi, vi = d$vi, method = "FE")$b
    }
    pooled.pts = data.frame(yi = c(est.N, est.all), sei = c(0, 
        0))
    just_signif_est = function(.sei) .sei * qnorm(1 - alpha.select/2)
    if (favor.positive == TRUE) 
        sl = 1/qnorm(1 - alpha.select/2)
    if (favor.positive == FALSE) 
        sl = -1/qnorm(1 - alpha.select/2)
    int = 0
    colors = c("darkgray", "orange")
    p.funnel = ggplot(data = d, aes(x = d$yi, y = d$sei, color = d$affirm))
    if (plot.pooled == TRUE) {
        p.funnel = p.funnel + geom_point(data = pooled.pts, aes(x = pooled.pts$yi, 
            y = pooled.pts$sei), size = 4, shape = 5, fill = NA, 
            color = c(colors[1], "black")) + geom_point(data = pooled.pts, 
            aes(x = pooled.pts$yi, y = pooled.pts$sei), size = 4, 
            shape = 18, color = c(colors[1], "black"), alpha = 1) + 
            geom_hline(yintercept = 0) + geom_abline(slope = sl, 
            intercept = int, color = "gray")
    }
    p.funnel = p.funnel + geom_point(size = 3, alpha = 0.3) + 
        geom_point(size = 3, shape = 1) + scale_color_manual(values = colors) + 
        xlab(xlab) + ylab(ylab) + scale_x_continuous(limits = c(xmin, 
        xmax)) + scale_y_continuous(limits = c(ymin, ymax)) + 
        theme_classic() + theme(legend.title = element_blank())
    plot(p.funnel)
}


significance_funnel(
  ma_data$d_calc,
  ma_data$d_var_calc,
  xmin = min(ma_data$d_calc),
  xmax = max(ma_data$d_calc),
  ymin = 0,
  ymax = max(sqrt(ma_data$d_var_calc)),
  xlab = "Estimate Effect Size (Cohen's d)",
  ylab = "Estimated standard error",
  favor.positive = TRUE,
  est.all = all_estimate,
  est.N = affirm_estimate,
  alpha.select = 0.05,
  plot.pooled = TRUE
)

```


# Main model results  {.tabset}

The tables below show the estimates for all single-moderator models reported in the main text. The tables present point estimates for the model parameters and their 95% confidence intervals (i.e., [lower bound, upper bound]). Asterisks (*) indicate significance at the .05 level. For categorical variables, the base levels are represented as the first ones appeared in the parentheses. 

```{r}
MODERATORS <- c( "NULL", "mean_age_months","productive_vocab_median", "sentence_structure", "agent_argument_type", "patient_argument_type", "n_repetitions_sentence",  "stimuli_modality", "stimuli_actor", "transitive_event_type","intransitive_event_type", "presentation_type","character_identification", "practice_phase", "test_mass_or_distributed", "n_train_test_pair", "n_test_trial_per_pair" )



si_mod_print <- generate_moderator_df(MODERATORS, ma_data) 

```

## Mean age 
```{r}
convert_pretty_print_table(si_mod_print, "mean_age_months") %>%
  mutate(Parameter = case_when(
    Parameter == "Mean Age Months" ~ "Mean Age (months)",
    TRUE ~ Parameter
  )) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Median productive vocabulary size
```{r}
convert_pretty_print_table(si_mod_print, "productive_vocab_median") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Productive Vocab Median" ~ "Median productive vocabulary size", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Predicate Type
```{r}
convert_pretty_print_table(si_mod_print, "sentence_structure") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Sentence Structure" ~ "Predicate type (Intransitive / Transitive)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Noun phrase type
```{r}
convert_pretty_print_table(si_mod_print, "agent_argument_type") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Agent Argument Type" ~ "Noun phrase type (Noun / Pronoun)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Character identification phase
```{r}
convert_pretty_print_table(si_mod_print, "character_identification") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Character Identification" ~ "Character identification phase \n (No / Yes)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Practice phase 
```{r}
convert_pretty_print_table(si_mod_print, "practice_phase") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Practice Phase" ~ "Practice phase \n (No / Yes)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Synchronicity 
```{r}
convert_pretty_print_table(si_mod_print, "presentation_type") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Presentation Type" ~ "Synchronicity \n (Asynchronous / Simultaneous)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Testing structure 
```{r}
convert_pretty_print_table(si_mod_print, "test_mass_or_distributed") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "Test Mass Or Distributed" ~ "Testing Procedure Structure \n (Distributed / Mass)", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```

## Number of sentence repetitions
```{r}
convert_pretty_print_table(si_mod_print, "n_repetitions_sentence") %>% 
  mutate(
    Parameter = case_when(
    Parameter == "N Repetitions Sentence" ~ "Number of sentence repetitions", 
    TRUE ~ Parameter)
  ) %>% 
  kable() %>% 
  kable_styling(font_size = 12)
```


# Models with methodological moderators and theoretical moderators {.tabset}
Syntactic bootstrapping studies differ in their implementational details. Here we examine the extent to which influences of the theoretical moderators can be accounted for by the methodological factors. The tables below present the results of models that include a single theoretical moderator along with all the methodological moderators in additive models. The row corresponding to the theoretical moderator is highlighted in yellow.   The effect of the theoretical moderators is qualitatively identical to the models without the methodological moderators included.



## With age
```{r}
age_method_m <- fit_method_model("mean_age_months", ma_data)
age_print_table <- print_method_model(age_method_m) %>% 
  mutate(Parameter = case_when(
    Parameter == "Mean Age Months" ~ "Mean Age (months)", 
    TRUE ~ Parameter
  ))
age_print_table %>% 
  kable() %>% 
  kable_styling(font_size = 12) %>% 
  row_spec(which(age_print_table$Parameter == "Mean age (months)"),bold = T, color = "black", background = "#f5deb3")


```

## With productive vocabulary size 
```{r}
vocab_method_m <- fit_method_model("productive_vocab_median", ma_data)
vocab_print_table <- print_method_model(vocab_method_m) 
vocab_print_table %>% 
  kable() %>% 
  kable_styling(font_size = 12) %>% 
  row_spec(which(vocab_print_table$Parameter == "Median productive vocabulary size"),bold = T,  color = "black", background = "#f5deb3")

```

## With predicate type
```{r}
sentence_structure_method_m <- fit_method_model("sentence_structure", ma_data)
sentence_structure_print_table <- print_method_model(sentence_structure_method_m)
sentence_structure_print_table %>% 
  kable() %>% 
  kable_styling(font_size = 12) %>% 
  row_spec(which(sentence_structure_print_table$Parameter == "Predicate type (Intransitive / Transitive)"),bold = T, color = "black", background = "#f5deb3")

```

## With Noun phrase type
```{r}
argument_type_method_m <- fit_method_model("agent_argument_type", ma_data)
argument_type_print <- print_method_model(argument_type_method_m) 

argument_type_print%>% 
  kable() %>% 
  kable_styling(font_size = 12) %>% 
  row_spec(which(argument_type_print$Parameter == "Noun phrase type (Noun / Pronoun)"),bold = T, color = "black", background = "#f5deb3")
```






# Variability in visual stimuli as a function of age

To assess the relationship between visual stimuli complexity and participant age, we collected sample visual stimuli for each condition in our sample  Schematic illustrations of the visual stimuli were used when the actual screenshots were not provided. Screenshots of the text descriptions of the events were used when the visual stimuli were unavailable. Note that because some papers' publishers converted to the visual stimuli to black-and-white, we decided to grayscale all visual stimuli for easier visual comparison.  

With the exception of studies from one paper targeting the youngest participants in our sample (Jin, 2015), there was little systematic variability in the complexity of visual stimuli as a function of age. This suggests that adaptation of visual stimuli to participants' age is unlikely to be the cause of the lack of developmental change of the strength of the effect. 


```{r}

ma_data <- read_csv(DATA_PATH) %>% 
  mutate(row_id = 1:n()) %>%
  rowwise() %>%
  mutate(stim_path = paste(unique_id, expt_num, sentence_structure,sep = "_"),
         stim_path = paste0(here("resources/stimuli_forplot/"), stim_path, ".png"),
         stim_path = str_replace(stim_path, "yuan2012_3_", "yuan2012_3simple_"))


age_model <- rma.mv(d_calc ~ mean_age, V = d_var_calc,
                      random = ~ 1 | short_cite/same_infant/row_id,
                      method = "REML",
                      data = ma_data )


```

```{r, fig.width = 10, fig.height = 8}
ma_data_with_predictions <- predict(age_model) %>%
  as.data.frame() %>%
  bind_cols(ma_data) %>% 
  mutate(
    mean_age_months = mean_age / 30.44
  )

ggplot(ma_data_with_predictions, aes(x = mean_age_months, y = d_calc)) +
  geom_image(aes(image = stim_path)) +
  geom_smooth(method = "lm") +
  #geom_smooth(method = "lm", data = ma_data_with_predictions, aes(x = mean_age, y = pred)) +
  scale_x_continuous(breaks = seq(0, 48, by = 12), limits = c(9, 48)) +
  geom_hline(aes(yintercept = 0), color = "black", linetype = "dashed") +
  ylab("Effect Size (d)") +
  xlab("Mean age (months)")
```





# Power analysis for experiment 
```{r}

pwr_no_mod_model <- rma.mv(d_calc~1, 
                            V  = d_var_calc,
                            random = ~ 1 |short_cite/same_infant/row_id, 
                            
                  data = ma_data, method = "REML")

es_stimate = pwr_no_mod_model$b[1]


mean_sample_size <- mean(ma_data$n_1)

d_pwr <- pwr_no_mod_model$b[,1][["intrcpt"]]
pwr_80 <- pwr::pwr.p.test(
  h = d_pwr, 
  sig.level = .05, 
  power = .8)$n
current_power <- pwr::pwr.p.test(h = d_pwr, 
           n = mean_sample_size, 
           sig.level = .05
           )$power



```

We conducted a power analysis using the `pwr` package (Champely et al., 2018). The x-axis represents the number of participants in each condition, and the y-axis represents the estimated power based on the power of estimated meta-analytic effect size (*d* = `r round(es_stimate,2)`). The horizontal black dotted line represents 80% power, and the vertical black dotted lines represent the number of participants needed to reach 80% power (*N* = `r round(pwr_80, 0)`). The red lines represents the current power (`r round(current_power * 100, 2)`%) based on the approximate mean sample sizes (*N* = `r round(mean_sample_size, 0)`) of the conditions included in the meta-analysis.

```{r}
max_n <- min(max(60,
                     pwr::pwr.p.test(h = d_pwr,
                                     sig.level = .05,
                                     power = .9)$n),
                 200)

pwrs <- data.frame(ns = seq(5, max_n, 5),
                   ps = pwr::pwr.p.test(h = d_pwr,
                                        n = seq(5, max_n, 5),
                   sig.level = .05)$power,
                   stringsAsFactors = FALSE)

qplot(ns, ps, geom = c("point","line"),
          data = pwrs) +
      geom_hline(yintercept = .8, lty = 2) +
      geom_vline(xintercept = pwr_80, lty = 3) +
      geom_hline(yintercept = current_power, lty = 2, color = "red") +
      geom_vline(xintercept = mean_sample_size, lty = 3, color = "red")+
      ylim(c(0,1)) +
      xlim(c(0,max_n)) +
      ylab("Power to reject the null at p < .05") +
      xlab("Number of participants (N)")
```

# Simulations of meta-analytic moderator power

For a number of the moderators we examined in our meta-analysis (e.g., noun phrase type), the average effect size did not statistically differ from zero. As such, in these cases, we failed to reject the null hypothesis.  This failure could be for two reasons: there is no true effect or there is an effect but we do not have the statistical power to detect it. We cannot determine which of these two reasons lead to the null moderating effects that we report, but we can examine our power  for moderator effects in the current meta-analysis (i.e. probability of rejecting a false null hypothesis).

There are not agreed-upon methods for calculating power for the multilevel mixed effect meta-analytic models we report in the paper, but methods do exist for mixed effect models (Hedges & Pigott, 2004). We conducted simulations of power for moderating effects using an implementation of these methods in R (metapower; Griffin, 2020).


```{r}

null_model <- rma(yi = d_calc, 
                    vi = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/row_id,
                     method = "REML",
                     data = ma_data )

i2 <- null_model$I2

mod_estimate <- mpower(
              effect_size = null_model$beta %>% as.numeric(),
              study_size = ma_data %>% select(n_1) %>% pull() %>% mean(),#questionable?
              k = nrow(ma_data),
              i2 = i2/100,
              es_type = "d",
              test_type = "two-tailed",
              p = 0.05,
              con_table = NULL
              )
```



```{r}
transitive_model <- rma(d_calc,
                             vi = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/row_id,
                     method = "REML",
                     data = filter(ma_data, sentence_structure == "transitive"))

intransitive_model <- rma(d_calc,
                             vi = d_var_calc,
                    random = ~ 1 | short_cite/same_infant/row_id,
                     method = "REML",
                     data = filter(ma_data, sentence_structure == "intransitive") )

n_study_transitive <- ma_data %>% ungroup() %>% filter( sentence_structure == "transitive") %>% distinct(plot_label) %>% count() %>% pull()
n_study_intransitive <- ma_data %>%  ungroup() %>% filter( sentence_structure == "intransitive") %>% distinct(plot_label) %>% count() %>% pull()
```

```{r}
# get actual MA params
actual_study_size = ma_data %>% select(n_1) %>% pull() %>% mean()
actual_k <- nrow(ma_data)/2
actual_i2 <- mean(c(intransitive_model$I2, c(transitive_model$I2)))

# function for getting power varying ES1 and k
power_sim_function <- function(es1, this_k, this_study_size, es2, this_i2){
      power <- mod_power(
              n_groups = 2,
              effect_sizes = c(es1, es2),
              study_size =  this_study_size,# this should be participants
              k = this_k,
              i2 = this_i2/100,
              es_type = "d",
              p = 0.05,
              con_table = NULL
              )$mod_power[[2]]
      
      data.frame(transitive_es = es1,
                 k = this_k,
                 estimated_power = power)
}

# do the thing
target_es <- c(seq(0, 1, .05))
k_factor <- c(1, 2, 5, 10)
sim_es <- cross_df(list(transitive = target_es, k = k_factor*actual_k))
#(ask abuot the map2_df))
estimated_power <- map2_df(
          sim_es$transitive, 
          sim_es$k,
          power_sim_function,
          actual_study_size,
          0,
          actual_i2) 
```

To estimate power for a categorical moderating effect, we varied the effect size difference between the two levels of the moderator (e.g., "pronoun" vs. "noun"), and the number of conditions (studies) present for each level in the meta-analysis. We estimated the between study variance empirically ($I^2$ = `r round(actual_i2, 2)`). 

The figure below shows the estimated power if our meta-analytic models for  moderator effects. Shown in purple is the estimated power, given the actual number of conditions present in our meta-analysis; other lines show estimated power if more conditions were present in our meta-analysis. The dashed line shows 80% power.

This analysis suggests that we have reasonably high power to detect medium-to-large effects size differences (at least $d$ = .5), but low power to detect smaller effect sizes. Thus, the null effects for moderating effects reported in the paper should be interpreted with caution: while these analyses provide strong evidence that  moderator effects are not large, it remains possible that some of the null effects we report are actually non-zero but small in magnitude. 


```{r}
# Plot the thing
plotting_data <-   estimated_power %>%
  mutate(k_ratio_print = case_when(
    k == actual_k*1 ~ "Actual # of conditions", 
    k == actual_k*2 ~ "x2 actual # of conditions", 
    k == actual_k*5 ~ "x5 actual # of conditions", 
    k == actual_k*10 ~ "x10 actual # of conditions"
  )) %>%
  mutate(k_ratio_print = fct_relevel(k_ratio_print, "x10 actual # of conditions", "x5 actual # of conditions", "x2 actual # of conditions", "Actual # of conditions"))

ggplot(plotting_data, aes(x = transitive_es, y = estimated_power, color = k_ratio_print, group = k_ratio_print)) +
  geom_hline(aes(yintercept = .8), linetype = 2) +
  geom_point() +
  geom_line() +
  xlab("Effect Size Difference") +
  annotate("text", x = .85, y = .83, label = "80% power") +
  ylab("Estimated Power") +
  ggtitle("Simulated power for moderator analyses") +
  theme_classic() +
  scale_color_discrete(name = "Number of conditions in MA",
                       labels = c(expression(paste(italic("x"), "10 actual # of conditions")), expression(paste(italic("x"), "5 actual # of conditions")),
                                  expression(paste(italic("x"), "2 actual # of conditions")), "Actual # of conditions")) +
  theme(legend.text.align = 0)
```





**References**

Champely, S., Ekstrom, C., Dalgaard, P., Gill, J., Weibelzahl, S., Anandkumar, A., ... & De Rosario, M. H. (2018). Package ‘pwr’. R package version, 1(2).

Griffin JW (2020). metapoweR: an R package for computing meta-analytic statistical power. R package version 0.2.1, https://CRAN.R-project.org/package=metapower.

Hedges, L. V., & Pigott, T. D. (2004). The power of statistical tests for moderators in meta-analysis. Psychological methods, 9(4), 426.

Hirsh-Pasek, K., Golinkoff, R. M., & Naigles, L. (1996). Young children’s use of syntactic frames to derive meaning. The origins of grammar: Evidence from early language comprehension, 123-158.

Jin, K. S. (2015). The role of syntactic and discourse information in verb learning (Doctoral dissertation, University of Illinois at Urbana-Champaign).

Mathur, M. B., & VanderWeele, T. J. (2020). Sensitivity analysis for publication bias in meta‐analyses. Journal of the Royal Statistical Society. Series C, Applied Statistics, 69(5), 1091.

Yuan, S., & Fisher, C. (2009). “Really? She blicked the baby?” Two-year-olds learn combinatorial facts about verbs by listening. Psychological science, 20(5), 619-626.
